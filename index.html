<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks</title>
	
 
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<style>
.container {
  position: relative;
}

.text-block {
  position: relative;
  top: 0px;
  right: 0px;
  margin-left: 5px;
  width: 97.5%;
  text-align: center;
  border-radius:10px 10px 0px 0px;
  border: 1px solid #787878;
  background-color: #787878;
  color: white;
  padding-left: 0px;
  padding-right: 0px;
  padding-top: 3px;
  padding-bottom: 3px;
}
</style>
</head>
<body>

<style>
  .uniform-image {
    width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  .small-image {
    width: 75%;     /* You can adjust this */
    height: auto;
    display: block;
    margin: 0 auto;
  }
</style>

<section class="hero">
  <div style="margin-bottom:-80px" class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks</h1>
		<!-- <h1 style="font-size:2vw"><font color="#ff0000"> <b> CVPR 2024 [Highlight, Top 2.8%] </b> </font></h1> <br> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a><sup>1</sup>,</span>
            <span class="author-block">
              Mohamed Elmoghany</a><sup>2</sup>,</span>
            <span class="author-block">
              Yohan Abeysinghe</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://feielysia.github.io">Junjie Fei</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sayannag.github.io/">Sayan Nag</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://salman-h-khan.github.io">Salman Khan</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.mohamed-elhoseiny.com">Mohamed Elhoseiny</a><sup>2</sup>,</span>
            <span class="author-block">
            </span>
      			<span class="author-block">
              <a href="https://www.cs.umd.edu/people/dmanocha/">Dinesh Manocha</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland,</span>
            <span class="author-block"><sup>2</sup>KAUST</span>
            <span class="author-block"><sup>3</sup>MBZUAI</span>
			      <span class="author-block"><sup>4</sup>Adobe</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
			  <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="./static/video/Video.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <video width="320" height="180" controls>
                  <source src="./static/video/Video.mp4" type="video/mp4" />
                  Your browser does not support the video tag.
                </video>
              </span> -->
			  <!--  Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
			  <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
				 -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
	<!-- TL;DR. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
		<center>
        <div class="content has-text-justified" style='width:750px'>
          <p>
            We propose MAGNET, a multi-agent framework that retrieves and reasons over audio-visual cues from multiple 
            videos to generate grounded, step-wise answers to complex queries.
          </p>
        </div>
		</center>
      </div>
    </div>
	
</div>




</section>
	
  <section class="columns is-vcentered interpolation-panel" width=100%>
  <div class="container is-max-desktop">
    <div class="hero-body" style='margin-top:-25px;margin-bottom:-25px'>
	<center>
      <h2 class="title is-3">MAGNET Overview</h2>
      <img src="./static/images/Main_System.png"
                 class="interpolation-image" width=80%/></center>
      <p class="content has-text-justified">
		Our approach MAGNET generates step-wise, grounded answers to complex audio-visual queries by first retrieving the 
    top-K relevant videos using AV-RAG, then processing them through dynamically instantiated audio-visual agents and a 
    meta-agent aggregator. A salient frame selector module adaptively filters modality-agnostic keyframes, enhancing reasoning 
    and generation quality. Please refer to Section 3 in the manuscript for more details.
      </p>
    </div>
  </div>
  </section>




  
  <section class="section">
  <div class="container is-max-desktop">
    
  <!-- TL;DR. -->
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Abstract</h2>
    <center>
        <div class="hero-body">
          <p class="content has-text-justified">
          Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with 
          real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video 
          question answering remain limited in scope, typically involving one clip per query, which falls short of representing 
          the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge 
          this gap, we introduce a <strong><i>novel task</i></strong> named <strong><i>AVHaystacksQA</i></strong>, where the goal 
          is to identify salient segments across different videos in response to a query and link them together to generate the 
          most informative answer. To this end, we present <strong><i>AVHaystacks</i></strong>, an audio-visual benchmark 
          comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal 
          grounding task. Additionally, we propose a model-agnostic, multi-agent framework <strong><i>Magnet</i></strong> to address 
          this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores 
          in QA task on our proposed <strong><i>AVHaystacks</i></strong>. To enable robust evaluation of multi-video retrieval and 
          temporal grounding for optimal response generation, we introduce two new metrics, <strong><i>StEM</i></strong>, which 
          captures alignment errors between a ground truth and a predicted step sequence and <strong><i>MTGS</i></strong>, to 
          facilitate balanced and interpretable evaluation of segment-level grounding performance.
		  </p>
        </div>
    </center>
      </div>
    </div>
  
</div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Dataset</h2>
      </div>
    </div>
  
</div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/teaser-dataset.jpeg" class="small-image" />
          <p class="content has-text-justified">
            The benchmark comprises 103 hours of video content from 500 video samples across 27 diverse
            categories, accompanied by carefully annotated QA pairs that temporally ground salient segments within the
            videos. To the best of our knowledge, this is the first benchmark of its kind, as no prior work provides
            multi-video linked audio-visual QA pairs
        </div>
      </div>
    </div>
  </div>
</section>

<div class="my-6"></div>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/teaser-task.jpeg" class="small-image" />
          <p class="content has-text-justified">
          AVHaystacksQA, and introduce AVHaystacks —a new benchmark
          consisting of 3100 audio-visual QA pairs drawn from videos across diverse domains (Fig. 1). This
          benchmark pushes the boundaries of video retrieval and reasoning by requiring models to navigate
          and reason over large-scale video collections. To the best of our knowledge, no existing benchmark
          systematically evaluates multi-video keypoint detection and reasoning capabilities.
          </p>  
        </div>
      </div>
    </div>
  </div>
</section>



  <!-- TL;DR. -->
<!-- TL;DR. -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Qualitative Results of MAGNET</h2>
      </div>
    </div>
  
</div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <center><h2 class="title is-3">Example 1</h2></center>
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/Qualitative-Results1.png" class="uniform-image" />
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="padding-top: 40px;">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <center><h2 class="title is-3">Example 2</h2></center>
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/Qualitative-Results2.png" class="uniform-image" />
        </div>
      </div>
    </div>
  </div>
</section>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">MAGNET Ablations</h2>
      </div>
    </div>
</div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/ablations.png" class="uniform-image" />
          <p class="content has-text-justified">
            Performance is generally best when both audio and visual modalities are used, highlighting the benefit of multi-modal information.
            Gemini-1.5-Pro consistently outperforms Qwen-2.5-Omni across all retrieval and response alignment scores metrics, indicating the benefits 
            of MAGNET in formulating coherent and information-rich responses.
            Results underscore the advantage of semantically guided sampling over uniform strategies, as SFS more effectively captures informative segments, 
            leading to better grounding, coherence, and human preference.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/gamma_vs_metrics_plots.pdf" class="uniform-image" />
          <p class="content has-text-justified">
          This demonstrates a steady rise in performance across all metrics as $\gamma$ increases, although a slight dip in performance 
          is observed at $\gamma$ = 25, notably in BLEU@4 and Human Eval for \ourframework + Qwen-2.5-Omni-FT, potentially indicate the 
          onset of overfitting or increased parameter sensitivity in that region. The varying magnitudes of the dip across metrics indicate 
          that the effect of $\gamma$ is not uniform across different aspects of model performance.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Main Results</h2>
      </div>
    </div>
</div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/grounding_evaluation.png" class="uniform-image" />
          <p class="content has-text-justified">
          Grounding evaluation and Step-wise error results on AVHaystack-50 and AVHaystack-Full datasets using MTGS
          and STEM (SM, SH, SO, SFP, SFN) metrics respectively and Retrieval Evaluation Scores on AVHaystack-50 and 
          AVHaystack-Full datasets.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body" style="margin-top:-25px; margin-bottom:-25px;">
    <div class="container is-max-desktop">
      <div class="container pt-3">
        <div class="has-text-centered">
          <img src="./static/images/main-table.png" class="uniform-image" />
          <p class="content has-text-justified">
          Our proposed MAGNET offers significant gains over baseline approaches (first section) and our adapted baselines 
          (second section) across multiple objective and subjective metrics on two dataset splits. B@4: BLEU@4, Cr: CIDEr, 
          H Eval: Human Evaluation. Closed source model: as a reference for upperbound
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top:0px">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chowdhury2024melfusion,
  author    = {Chowdhury, Sanjoy  and Elmoghany, Mohamed and Abeysinghe, Yohan and Fei, Junjie and Nag, Sayan and Khan, Salman and Elhoseiny, Mohamed and Manocha, Dinesh},
  title     = {MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks},
  booktitle = {},
  year      = {2025}
}
</code></pre>
  </div>
</section>


</body>
</html>
